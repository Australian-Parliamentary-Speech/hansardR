---
title: "Complete Hansard Data Processing Workflow"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 2
  github_document:
    toc: true
    toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Complete Hansard Data Processing Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE  # Now we can evaluate with sample data
)
```

# Processing Australian Parliamentary Hansard Data

The `hansardR` package provides a comprehensive toolkit for processing, validating, and analysing Australian Parliamentary Hansard CSV data. This vignette demonstrates the complete workflow from raw CSV files to a structured database ready for computational social science research.

## Overview

The package handles parliamentary data with the following structure:
- **Sessions**: Parliamentary sitting days
- **Members**: MPs with party affiliations and electorates  
- **Debates**: Major topics discussed in each session
- **Speeches**: Individual contributions, questions, answers, and interjections

## Setup

```{r setup}
library(hansardR)
library(dplyr)
library(DBI)

# Use sample data included with the package
sample_data_path <- system.file("extdata", "houseCSV", package = "hansardR")
db_path <- tempfile(fileext = ".db")  # Use temporary database for vignette

cat("Sample data location:", sample_data_path, "\n")
cat("Temporary database:", db_path, "\n")

# Show available sample data structure
if (dir.exists(sample_data_path)) {
  cat("\nAvailable sample years:\n")
  year_dirs <- list.dirs(sample_data_path, recursive = FALSE, full.names = FALSE)
  for (year in year_dirs) {
    year_path <- file.path(sample_data_path, year)
    files <- list.files(year_path, pattern = "*.csv")
    cat(" ", year, ": ", length(files), " files\n")
    for (file in files) {
      cat("   -", file, "\n")
    }
  }
}
```

## Step 1: Database Creation

Create a new SQLite database with the standard Hansard schema:

```{r database-creation}
# Create database with standard schema
con <- create_hansard_database(db_path, overwrite = TRUE)

# Or connect to existing database
# con <- connect_hansard_database(db_path)
```

The database includes optimised indexes for:
- Temporal queries (by date, year)
- Member-based analysis (by party, electorate)
- Content analysis (full-text search ready)
- Debate structure (hierarchical organisation)

## Step 2: File Discovery and Validation

Before processing, validate your CSV files to ensure data integrity:

```{r file-discovery}
# Find sample CSV files included with the package
sample_years <- list.dirs(sample_data_path, recursive = FALSE, full.names = TRUE)
sample_years <- sample_years[grepl("\\d{4}$", basename(sample_years))]  # Only year directories

# Get all sample files
sample_files <- c()
for (year_dir in sample_years) {
  files <- list.files(year_dir, pattern = "*_edit_step7.csv", full.names = TRUE)
  sample_files <- c(sample_files, files)
}

cat("Found", length(sample_files), "sample files across", length(sample_years), "years:\n")
cat("Years available:", paste(basename(sample_years), collapse = ", "), "\n")

# Show file details
for (file in sample_files) {
  year <- basename(dirname(file))
  filename <- basename(file)
  size_kb <- round(file.size(file) / 1024, 1)
  cat(" ", year, "/", filename, " (", size_kb, " KB)\n", sep = "")
}
```

### File Structure Validation

```{r validation}
# Validate file structures
validation_results <- validate_csv_batch(sample_files)

# View validation summary
print(validation_results)

# Check for issues
problems <- validation_results[!validation_results$valid, ]
if (nrow(problems) > 0) {
  cat("Files with issues:\n")
  print(problems[c("filename", "issues", "error")])
}

# Files ready for import
valid_files <- validation_results$file_path[validation_results$valid]
cat("\nValid files ready for import:", length(valid_files), "\n")
```

## Step 3: Single File Import

Start by importing a single file to test the workflow:

```{r single-import}
# Import one file for testing
if (length(valid_files) > 0) {
  test_file <- valid_files[1]
  cat("Testing import with:", basename(test_file), "\n")
  
  success <- import_hansard_file(test_file, con, validate = TRUE)
  
  if (success) {
    cat("âœ“ Test import successful!\n")
    
    # Check what was imported
    stats <- get_database_stats(con)
    cat("Database now contains:\n")
    cat("  Sessions:", stats$sessions, "\n")
    cat("  Members:", stats$members, "\n")
    cat("  Speeches:", stats$speeches, "\n")
  }
}
```

## Step 4: Batch Processing

### Process a Single Year

```{r year-import}
# Process sample years
sample_year_dirs <- list.dirs(sample_data_path, recursive = FALSE, full.names = TRUE)
sample_year_dirs <- sample_year_dirs[grepl("\\d{4}$", basename(sample_year_dirs))]

for (year_dir in sample_year_dirs) {
  year_name <- basename(year_dir)
  cat("\n=== Processing", year_name, "===\n")
  
  # Show what files we're about to process
  csv_files <- list.files(year_dir, pattern = "*_edit_step7.csv", full.names = TRUE)
  cat("Files to process:\n")
  for (file in csv_files) {
    cat("  -", basename(file), "\n")
  }
  
  year_results <- import_hansard_year(
    year_dir, 
    con, 
    pattern = "*_edit_step7.csv",
    validate = TRUE,
    progress = FALSE  # Disable progress bar in vignette
  )
  
  # Show results
  if (nrow(year_results) > 0) {
    successful <- sum(year_results$success)
    total <- nrow(year_results)
    cat("Result: Imported", successful, "out of", total, "files successfully\n")
    
    if (successful < total) {
      failed_files <- year_results$filename[!year_results$success]
      cat("Failed files:", paste(failed_files, collapse = ", "), "\n")
    }
  }
}
```

### Process Multiple Years

```{r multi-year-import}
# Process both available years in the sample data
available_years <- basename(list.dirs(sample_data_path, recursive = FALSE, full.names = FALSE))
available_years <- available_years[grepl("^\\d{4}$", available_years)]

cat("Processing all available sample years:", paste(available_years, collapse = ", "), "\n")

all_results <- list()

for (year in available_years) {
  year_path <- file.path(sample_data_path, year)
  
  if (dir.exists(year_path)) {
    cat("\n=== Processing", year, "===\n")
    
    # Show what we're processing
    csv_files <- list.files(year_path, pattern = "*_edit_step7.csv")
    cat("Found", length(csv_files), "CSV files in", year, "\n")
    
    year_results <- import_hansard_year(
      year_path, 
      con,
      validate = TRUE,
      force_reimport = FALSE,  # Skip existing sessions
      progress = FALSE
    )
    
    all_results[[year]] <- year_results
    
    # Show summary
    if (nrow(year_results) > 0) {
      successful <- sum(year_results$success)
      total <- nrow(year_results)
      cat("Summary:", successful, "/", total, "files imported successfully\n")
    }
  } else {
    cat("Directory not found:", year_path, "\n")
  }
}

# Combine results
if (length(all_results) > 0) {
  combined_results <- do.call(rbind, all_results)
  cat("\n=== Overall Import Summary ===\n")
  total_files <- nrow(combined_results)
  total_successful <- sum(combined_results$success)
  cat("Total files processed:", total_files, "\n")
  cat("Successfully imported:", total_successful, "\n")
  cat("Success rate:", round(100 * total_successful / total_files, 1), "%\n")
}
```

## Step 5: Data Exploration

### Database Statistics

```{r stats}
# Get comprehensive database statistics
stats <- get_database_stats(con)

cat("=== Database Statistics ===\n")
cat("Sessions:", stats$sessions, "\n")
cat("Members:", stats$members, "\n")
cat("Debates:", stats$debates, "\n")
cat("Speeches:", stats$speeches, "\n")
cat("Date range:", as.character(stats$date_range[1]), "to", as.character(stats$date_range[2]), "\n")
cat("Average speech length:", round(stats$avg_length, 1), "characters\n")

cat("\nSpeech types:\n")
cat("  Questions:", stats$questions, "\n")
cat("  Answers:", stats$answers, "\n")
cat("  Speeches:", stats$speeches, "\n")
cat("  Interjections:", stats$interjections, "\n")
```

### Top Speakers Analysis

```{r top-speakers}
# Get most active speakers
top_speakers <- get_top_speakers(con, limit = 15)
print(top_speakers)

# Party breakdown
party_activity <- top_speakers |>
  group_by(party) |>
  summarise(
    members = n(),
    total_speeches = sum(total_speeches),
    avg_speeches_per_member = round(mean(total_speeches), 1),
    .groups = "drop"
  ) |>
  arrange(desc(total_speeches))

print(party_activity)
```

## Step 6: Advanced Queries with dplyr

Get table references for direct dplyr querying:

```{r dplyr-setup}
# Get table references
tbls <- get_hansard_tables(con)

# Show table structure
cat("Available tables:\n")
for (name in names(tbls)) {
  cat(" ", name, "\n")
}
```

### Temporal Analysis

```{r temporal-analysis}
# Questions over time by party (using our sample data)
questions_by_party_year <- tbls$speeches |>
  filter(is_question == 1) |>
  left_join(tbls$members, by = "member_id") |>
  left_join(tbls$sessions, by = "session_id") |>
  count(party, year, sort = TRUE) |>
  collect()

if (nrow(questions_by_party_year) > 0) {
  cat("Questions by party and year in sample data:\n")
  print(questions_by_party_year)
} else {
  cat("No questions found in sample data\n")
}

# Session-level activity (all speech types)
session_activity <- tbls$speeches |>
  left_join(tbls$sessions, by = "session_id") |>
  group_by(session_date, year) |>
  summarise(
    total_speeches = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    interjections = sum(is_interjection, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(session_date) |>
  collect()

cat("\nSession-level activity:\n")
print(session_activity)
```

### Content Analysis

```{r content-analysis}
# Average speech length by party (using sample data)
speech_length_by_party <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  filter(!is.na(party)) |>  # Only include rows with party information
  group_by(party) |>
  summarise(
    speech_count = n(),
    avg_length = round(mean(content_length, na.rm = TRUE), 1),
    median_length = round(median(content_length, na.rm = TRUE), 1),
    total_words = sum(content_length, na.rm = TRUE),
    .groups = "drop"
  ) |>
  filter(speech_count >= 1) |>  # Include all parties in sample data
  arrange(desc(avg_length)) |>
  collect()

cat("Speech length statistics by party:\n")
print(speech_length_by_party)

# Most active debates in sample data
popular_debates <- tbls$speeches |>
  left_join(tbls$debates, by = "debate_id") |>
  filter(!is.na(debate_title)) |>
  count(debate_title, sort = TRUE) |>
  collect()

cat("\nMost discussed topics in sample data:\n")
print(popular_debates)

# Speech type distribution
speech_types <- tbls$speeches |>
  summarise(
    total_records = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    speeches = sum(is_speech, na.rm = TRUE),
    interjections = sum(is_interjection, na.rm = TRUE),
    stage_directions = sum(is_stage_direction, na.rm = TRUE)
  ) |>
  collect()

cat("\nSpeech type distribution in sample data:\n")
print(speech_types)
```

### Member-Specific Analysis

```{r member-analysis}
# Most active members in sample data
sample_speakers <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  left_join(tbls$sessions, by = "session_id") |>
  filter(!is.na(full_name)) |>  # Only include identified speakers
  group_by(full_name, party, electorate) |>
  summarise(
    sessions_active = n_distinct(session_id),
    total_contributions = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    speeches = sum(is_speech, na.rm = TRUE),
    avg_speech_length = round(mean(content_length, na.rm = TRUE), 1),
    .groups = "drop"
  ) |>
  arrange(desc(total_contributions)) |>
  collect()

cat("Member activity in sample data:\n")
print(sample_speakers)

# Party representation in sample
party_summary <- tbls$members |>
  filter(!is.na(party)) |>
  count(party, sort = TRUE) |>
  collect()

cat("\nParty representation in sample data:\n")
print(party_summary)
```

## Step 7: Text Search and Analysis

### Simple Content Search

```{r content-search}
# Search for specific topics using database-friendly approach
# Note: grepl() doesn't work with databases, so we use direct SQL
climate_mentions <- DBI::dbGetQuery(con, "
  SELECT 
    m.full_name,
    m.party, 
    s.session_date,
    sp.content_length,
    SUBSTR(sp.content, 1, 100) || '...' as content_preview
  FROM speeches sp
  LEFT JOIN members m ON sp.member_id = m.member_id  
  LEFT JOIN sessions s ON sp.session_id = s.session_id
  WHERE LOWER(sp.content) LIKE '%climate%'
  ORDER BY s.session_date DESC
  LIMIT 10
")

if (nrow(climate_mentions) > 0) {
  cat("Recent 'climate' mentions:\n")
  print(climate_mentions)
} else {
  cat("No 'climate' mentions found in current dataset\n")
  
  # Try a broader search
  any_mentions <- DBI::dbGetQuery(con, "
    SELECT COUNT(*) as total_speeches
    FROM speeches 
    WHERE content IS NOT NULL AND LENGTH(content) > 10
  ")
  cat("Total speeches with content:", any_mentions$total_speeches, "\n")
}

# Example of other search terms you might use
search_terms <- c("economy", "health", "education", "budget")
for (term in search_terms) {
  count_query <- paste0("
    SELECT COUNT(*) as count 
    FROM speeches 
    WHERE LOWER(content) LIKE '%", tolower(term), "%'
  ")
  
  result <- DBI::dbGetQuery(con, count_query)
  if (result$count > 0) {
    cat("Found", result$count, "mentions of '", term, "'\n")
  }
}
```

### Alternative Database-Friendly Search Pattern

For better performance with large databases, you can also use this pattern:

```{r content-search-efficient}
# More efficient search for large databases
# Search with LIMIT in SQL rather than collecting all results
climate_search <- DBI::dbGetQuery(con, "
  SELECT 
    m.full_name,
    m.party, 
    s.session_date,
    sp.content_length
  FROM speeches sp
  LEFT JOIN members m ON sp.member_id = m.member_id  
  LEFT JOIN sessions s ON sp.session_id = s.session_id
  WHERE sp.content LIKE '%climate%'
  ORDER BY s.session_date DESC
  LIMIT 10
")

if (nrow(climate_search) > 0) {
  cat("Climate mentions using direct SQL:\n")
  print(climate_search)
} else {
  cat("No climate mentions found\n")
}
```

### Topic-Based Analysis

```{r topic-analysis}
# Questions vs Answers by party
qa_balance <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  group_by(party) |>
  summarise(
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    question_answer_ratio = round(
      sum(is_question, na.rm = TRUE) / pmax(sum(is_answer, na.rm = TRUE), 1), 2
    ),
    .groups = "drop"
  ) |>
  filter(questions + answers >= 10) |>
  arrange(desc(question_answer_ratio)) |>
  collect()

print(qa_balance)
```

## Step 8: Data Export for External Analysis

### Export to CSV for R Analysis

```{r data-export}
# Export member statistics
member_stats <- get_top_speakers(con, limit = 100)
write.csv(member_stats, "member_activity_stats.csv", row.names = FALSE)

# Export session summary
session_summary <- tbls$sessions |>
  left_join(
    tbls$speeches |> 
      group_by(session_id) |> 
      summarise(
        speech_count = n(),
        question_count = sum(is_question, na.rm = TRUE),
        avg_speech_length = round(mean(content_length, na.rm = TRUE), 1),
        .groups = "drop"
      ),
    by = "session_id"
  ) |>
  collect()

write.csv(session_summary, "session_summary.csv", row.names = FALSE)

cat("Data exported to CSV files for external analysis\n")
```

## Step 9: Database Maintenance

### Check Import Status

```{r maintenance}
# Check for any failed imports
if (exists("combined_results")) {
  failed_imports <- combined_results[!combined_results$success, ]
  
  if (nrow(failed_imports) > 0) {
    cat("Files that failed to import:\n")
    print(failed_imports[c("filename", "processed_at")])
    
    # Retry failed imports
    cat("\nRetrying failed imports...\n")
    retry_results <- import_hansard_batch(
      failed_imports$file_path, 
      con, 
      validate = TRUE,
      force_reimport = TRUE
    )
    print(retry_results)
  } else {
    cat("All files imported successfully!\n")
  }
}
```

### Database Optimization

```{r optimization}
# Get database file size
if (file.exists(db_path)) {
  db_size_mb <- round(file.size(db_path) / 1024^2, 1)
  cat("Database size:", db_size_mb, "MB\n")
}

# Optimize database (SQLite maintenance)
DBI::dbExecute(con, "VACUUM;")
DBI::dbExecute(con, "ANALYZE;")

cat("Database optimized\n")
```

## Cleanup

```{r cleanup}
# Close database connection when finished
dbDisconnect(con)
cat("Database connection closed\n")
```

## Next Steps

With your Hansard data now in a structured database, you can:

1. **Conduct longitudinal analysis** of parliamentary discourse
2. **Perform text mining** and sentiment analysis on speech content
3. **Analyse party dynamics** and question-answer patterns
4. **Study member behaviour** across different topics and time periods
5. **Export data** for use with other text analysis tools

## Troubleshooting

### Common Issues

**File validation failures:**
- Check file encoding (should be UTF-8)
- Verify column names match expected schema
- Ensure date format in filename is YYYY-MM-DD

**Import errors:**
- Use `force_reimport = TRUE` to overwrite existing sessions
- Check database permissions and disk space
- Validate file structure before import

**Memory issues with large datasets:**
- Process years individually rather than all at once
- Use `progress = FALSE` to reduce overhead
- Close and reconnect database periodically for long imports

**Query performance:**
- Database includes optimized indexes for common queries
- Use `collect()` only when you need data in R
- Filter early in dplyr chains to reduce data transfer

For additional help, see the package documentation: `help(package = "hansardR")`
