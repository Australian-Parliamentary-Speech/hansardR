---
title: "Process Hansard Data from a Single Directory"
description: >
  Complete workflow for processing all Hansard CSV files from a single directory.
  Includes database creation, file validation, batch import, and comprehensive analysis examples.
output: rmarkdown::html_vignette
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE  # Set to TRUE only when running locally with data files
)
```

# Process Hansard Data from a Single Directory

This article demonstrates how to process all Hansard CSV files from a single directory using real data paths. This approach is ideal when you have all your CSV files in one location, regardless of whether they span multiple years or sessions.

## Setup

```{r setup}
library(hansardR)
library(dplyr)
library(DBI)

# Set your data directory (only place that needs changing)
data_dir <- '/Users/kbenoit/Library/CloudStorage/OneDrive-AustralianNationalUniversity/ARC Text project - General/Output files/HouseCSV/2025/'
db_path <- "~/tmp/hansard_2025.db"

# Check if directory exists
if (!dir.exists(data_dir)) {
  stop("Data directory not found: ", data_dir)
}
```

## Step 1: Database Creation

Create a new SQLite database with the standard Hansard schema:

```{r database-creation}
# Create database with standard schema
con <- create_hansard_database(db_path, overwrite = TRUE)

# Or connect to existing database
# con <- connect_hansard_database(db_path)
```


## Step 2: File Discovery and Validation

Before processing, validate your CSV files to ensure data integrity:

```{r file-discovery}
# Find all CSV files in the data directory
csv_files <- list.files(data_dir, pattern = "*_edit_step7\\.csv$", full.names = TRUE)

cat("Found", length(csv_files), "CSV files in", basename(data_dir), ":\n")

# Show file details
for (file in csv_files) {
  filename <- basename(file)
  size_kb <- round(file.size(file) / 1024, 1)
  cat(" -", filename, " (", size_kb, " KB)\n", sep = "")
}

if (length(csv_files) == 0) {
  stop("No CSV files found matching pattern '*_edit_step7.csv' in: ", data_dir)
}
```

### File Structure Validation

```{r validation}
# Validate file structures
validation_results <- validate_csv_batch(csv_files)

# View validation summary
print(validation_results)

# Check for issues
problems <- validation_results[!validation_results$valid, ]
if (nrow(problems) > 0) {
  cat("Files with issues:\n")
  print(problems[c("filename", "issues", "error")])
}

# Files ready for import
valid_files <- validation_results$file_path[validation_results$valid]
cat("\nValid files ready for import:", length(valid_files), "\n")
```

## Step 3: Single File Import

Start by importing a single file to test the workflow:

```{r single-import}
# Import one file for testing
if (length(valid_files) > 0) {
  test_file <- valid_files[1]
  cat("Testing import with:", basename(test_file), "\n")
  
  success <- import_hansard_file(test_file, con, chamber = "house", validate = TRUE)
  
  if (success) {
    cat("âœ“ Test import successful!\n")
    
    # Check what was imported
    stats <- get_database_stats(con)
    cat("Database now contains:\n")
    cat("  Sessions:", stats$sessions, "\n")
    cat("  Members:", stats$members, "\n")
    cat("  Speeches:", stats$speeches, "\n")
  }
}
```

## Step 4: Batch Processing

### Process All Files in Directory

```{r batch-import}
# Process all valid files using batch import
cat("=== Processing", length(valid_files), "files ===\n")

batch_results <- import_hansard_batch(
  valid_files,
  con,
  chamber = "house",
  validate = TRUE,
  progress = FALSE  # Disable progress bar in vignette
)

# Show results
if (nrow(batch_results) > 0) {
  successful <- sum(batch_results$success)
  total <- nrow(batch_results)
  cat("Result: Imported", successful, "out of", total, "files successfully\n")
  
  if (successful < total) {
    failed_files <- batch_results$filename[!batch_results$success]
    cat("Failed files:", paste(failed_files, collapse = ", "), "\n")
    
    # Show failed files (no detailed error info available from batch import)
    failures <- batch_results[!batch_results$success, c("filename", "processed_at")]
    print(failures)
  }
} else {
  cat("No files processed\n")
}
```

### Alternative: Process Using Directory Function

For processing files by year directory, you can also use the `import_hansard_year()` function:

```{r year-import-alternative}
# Alternative approach: use import_hansard_year() if your files are organized by year
# This is useful when you have subdirectories like: 2024/, 2025/, etc.

year_results <- import_hansard_year(
  data_dir,
  con,
  chamber = "house",
  pattern = "*_edit_step7.csv",
  validate = TRUE,
  force_reimport = FALSE,  # Skip existing sessions
  progress = FALSE
)

if (nrow(year_results) > 0) {
  successful <- sum(year_results$success)
  total <- nrow(year_results)
  cat("Year import summary:", successful, "/", total, "files imported successfully\n")
  
  # Store results for later use
  import_results <- year_results
} else {
  # Use batch results if year import wasn't used
  import_results <- batch_results
}
```

## Step 5: Data Exploration

### Database Statistics

```{r stats}
# Get comprehensive database statistics
stats <- get_database_stats(con)

cat("=== Database Statistics ===\n")
cat("Sessions:", stats$sessions, "\n")
cat("Members:", stats$members, "\n")
cat("Debates:", stats$debates, "\n")
cat("Speeches:", stats$speeches, "\n")
cat("Date range:", as.character(stats$date_range[1]), "to", as.character(stats$date_range[2]), "\n")
cat("Average speech length:", round(stats$avg_length, 1), "characters\n")

cat("\nSpeech types:\n")
cat("  Questions:", stats$questions, "\n")
cat("  Answers:", stats$answers, "\n")
cat("  Speeches:", stats$speeches, "\n")
cat("  Interjections:", stats$interjections, "\n")
```

### Top Speakers Analysis

```{r top-speakers}
# Get most active speakers
top_speakers <- get_top_speakers(con, limit = 15)
print(top_speakers)

# Party breakdown
party_activity <- top_speakers |>
  group_by(party) |>
  summarise(
    members = n(),
    total_speeches = sum(total_speeches),
    avg_speeches_per_member = round(mean(total_speeches), 1),
    .groups = "drop"
  ) |>
  arrange(desc(total_speeches))

print(party_activity)
```

## Step 6: Advanced Queries with dplyr

Get table references for direct dplyr querying:

```{r dplyr-setup}
# Get table references
tbls <- get_hansard_tables(con)

# Show table structure
cat("Available tables:\n")
for (name in names(tbls)) {
  cat(" ", name, "\n")
}
```

### Temporal Analysis

```{r temporal-analysis}
# Questions over time by party (using our sample data)
questions_by_party_year <- tbls$speeches |>
  filter(is_question == 1) |>
  left_join(tbls$members, by = "member_id") |>
  left_join(tbls$sessions, by = "session_id") |>
  count(party, year, sort = TRUE) |>
  collect()

if (nrow(questions_by_party_year) > 0) {
  cat("Questions by party and year in sample data:\n")
  print(questions_by_party_year)
} else {
  cat("No questions found in sample data\n")
}

# Session-level activity (all speech types)
session_activity <- tbls$speeches |>
  left_join(tbls$sessions, by = "session_id") |>
  group_by(session_date, year) |>
  summarise(
    total_speeches = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    interjections = sum(is_interjection, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(session_date) |>
  collect()

cat("\nSession-level activity:\n")
print(session_activity)
```

### Content Analysis

```{r content-analysis}
# Average speech length by party (using sample data)
speech_length_by_party <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  filter(!is.na(party)) |>  # Only include rows with party information
  group_by(party) |>
  summarise(
    speech_count = n(),
    avg_length = round(mean(content_length, na.rm = TRUE), 1),
    median_length = round(median(content_length, na.rm = TRUE), 1),
    total_words = sum(content_length, na.rm = TRUE),
    .groups = "drop"
  ) |>
  filter(speech_count >= 1) |>  # Include all parties in sample data
  arrange(desc(avg_length)) |>
  collect()

cat("Speech length statistics by party:\n")
print(speech_length_by_party)

# Most active debates in sample data
popular_debates <- tbls$speeches |>
  left_join(tbls$debates, by = "debate_id") |>
  filter(!is.na(debate_title)) |>
  count(debate_title, sort = TRUE) |>
  collect()

cat("\nMost discussed topics in sample data:\n")
print(popular_debates)

# Speech type distribution
speech_types <- tbls$speeches |>
  summarise(
    total_records = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    speeches = sum(is_speech, na.rm = TRUE),
    interjections = sum(is_interjection, na.rm = TRUE),
    stage_directions = sum(is_stage_direction, na.rm = TRUE)
  ) |>
  collect()

cat("\nSpeech type distribution in sample data:\n")
print(speech_types)
```

### Member-Specific Analysis

```{r member-analysis}
# Most active members in sample data
sample_speakers <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  left_join(tbls$sessions, by = "session_id") |>
  filter(!is.na(full_name)) |>  # Only include identified speakers
  group_by(full_name, party, electorate) |>
  summarise(
    sessions_active = n_distinct(session_id),
    total_contributions = n(),
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    speeches = sum(is_speech, na.rm = TRUE),
    avg_speech_length = round(mean(content_length, na.rm = TRUE), 1),
    .groups = "drop"
  ) |>
  arrange(desc(total_contributions)) |>
  collect()

cat("Member activity in sample data:\n")
print(sample_speakers)

# Party representation in sample
party_summary <- tbls$members |>
  filter(!is.na(party)) |>
  count(party, sort = TRUE) |>
  collect()

cat("\nParty representation in sample data:\n")
print(party_summary)
```

## Step 7: Text Search and Analysis

### Simple Content Search

```{r content-search}
# Search for specific topics using database-friendly approach
# Note: grepl() doesn't work with databases, so we use direct SQL
climate_mentions <- DBI::dbGetQuery(con, "
  SELECT 
    m.full_name,
    m.party, 
    s.session_date,
    sp.content_length,
    SUBSTR(sp.content, 1, 100) || '...' as content_preview
  FROM speeches sp
  LEFT JOIN members m ON sp.member_id = m.member_id  
  LEFT JOIN sessions s ON sp.session_id = s.session_id
  WHERE LOWER(sp.content) LIKE '%climate%'
  ORDER BY s.session_date DESC
  LIMIT 10
")

if (nrow(climate_mentions) > 0) {
  cat("Recent 'climate' mentions:\n")
  print(climate_mentions)
} else {
  cat("No 'climate' mentions found in current dataset\n")
  
  # Try a broader search
  any_mentions <- DBI::dbGetQuery(con, "
    SELECT COUNT(*) as total_speeches
    FROM speeches 
    WHERE content IS NOT NULL AND LENGTH(content) > 10
  ")
  cat("Total speeches with content:", any_mentions$total_speeches, "\n")
}

# Example of other search terms you might use
search_terms <- c("economy", "health", "education", "budget")
for (term in search_terms) {
  count_query <- paste0("
    SELECT COUNT(*) as count 
    FROM speeches 
    WHERE LOWER(content) LIKE '%", tolower(term), "%'
  ")
  
  result <- DBI::dbGetQuery(con, count_query)
  if (result$count > 0) {
    cat("Found", result$count, "mentions of '", term, "'\n")
  }
}
```

### Alternative Database-Friendly Search Pattern

For better performance with large databases, you can also use this pattern:

```{r content-search-efficient}
# More efficient search for large databases
# Search with LIMIT in SQL rather than collecting all results
climate_search <- DBI::dbGetQuery(con, "
  SELECT 
    m.full_name,
    m.party, 
    s.session_date,
    sp.content_length
  FROM speeches sp
  LEFT JOIN members m ON sp.member_id = m.member_id  
  LEFT JOIN sessions s ON sp.session_id = s.session_id
  WHERE sp.content LIKE '%climate%'
  ORDER BY s.session_date DESC
  LIMIT 10
")

if (nrow(climate_search) > 0) {
  cat("Climate mentions using direct SQL:\n")
  print(climate_search)
} else {
  cat("No climate mentions found\n")
}
```

### Topic-Based Analysis

```{r topic-analysis}
# Questions vs Answers by party
qa_balance <- tbls$speeches |>
  left_join(tbls$members, by = "member_id") |>
  group_by(party) |>
  summarise(
    questions = sum(is_question, na.rm = TRUE),
    answers = sum(is_answer, na.rm = TRUE),
    question_answer_ratio = round(
      sum(is_question, na.rm = TRUE) / pmax(sum(is_answer, na.rm = TRUE), 1), 2
    ),
    .groups = "drop"
  ) |>
  filter(questions + answers >= 10) |>
  arrange(desc(question_answer_ratio)) |>
  collect()

print(qa_balance)
```

## Step 8: Data Export for External Analysis

### Export to CSV for R Analysis

```{r data-export}
# Export member statistics
member_stats <- get_top_speakers(con, limit = 100)
write.csv(member_stats, "member_activity_stats.csv", row.names = FALSE)

# Export session summary
session_summary <- tbls$sessions |>
  left_join(
    tbls$speeches |> 
      group_by(session_id) |> 
      summarise(
        speech_count = n(),
        question_count = sum(is_question, na.rm = TRUE),
        avg_speech_length = round(mean(content_length, na.rm = TRUE), 1),
        .groups = "drop"
      ),
    by = "session_id"
  ) |>
  collect()

write.csv(session_summary, "session_summary.csv", row.names = FALSE)

cat("Data exported to CSV files for external analysis\n")
```

## Step 9: Database Maintenance

### Check Import Status

```{r maintenance}
# Check for any failed imports from our processing
if (exists("import_results") && nrow(import_results) > 0) {
  failed_imports <- import_results[!import_results$success, ]
  
  if (nrow(failed_imports) > 0) {
    cat("Files that failed to import:\n")
    print(failed_imports[c("filename", "processed_at")])
    
    # Retry failed imports
    cat("\nRetrying failed imports...\n")
    retry_results <- import_hansard_batch(
      failed_imports$file_path, 
      con, 
      chamber = "house",
      validate = TRUE,
      force_reimport = TRUE,
      progress = FALSE
    )
    
    successful_retries <- sum(retry_results$success)
    cat("Retry results:", successful_retries, "/", nrow(retry_results), "files imported successfully\n")
  } else {
    cat("All files imported successfully!\n")
  }
} else {
  cat("No import results to check\n")
}
```

### Database Optimization

```{r optimization}
# Get database file size
if (file.exists(db_path)) {
  db_size_mb <- round(file.size(db_path) / 1024^2, 1)
  cat("Database size:", db_size_mb, "MB\n")
}

# Optimize database (SQLite maintenance)
DBI::dbExecute(con, "VACUUM;")
DBI::dbExecute(con, "ANALYZE;")

cat("Database optimized\n")
```

## Cleanup

```{r cleanup}
# Close database connection when finished
dbDisconnect(con)
cat("Database connection closed\n")
```

## Next Steps

With your Hansard data now in a structured database, you can:

1. **Conduct longitudinal analysis** of parliamentary discourse
2. **Perform text mining** and sentiment analysis on speech content
3. **Analyse party dynamics** and question-answer patterns
4. **Study member behaviour** across different topics and time periods
5. **Export data** for use with other text analysis tools

## Troubleshooting

### Common Issues

**File validation failures:**
- Check file encoding (should be UTF-8)
- Verify column names match expected schema
- Ensure date format in filename is YYYY-MM-DD

**Import errors:**
- Use `force_reimport = TRUE` to overwrite existing sessions
- Check database permissions and disk space
- Validate file structure before import

**Memory issues with large datasets:**
- Process years individually rather than all at once
- Use `progress = FALSE` to reduce overhead
- Close and reconnect database periodically for long imports

**Query performance:**
- Database includes optimized indexes for common queries
- Use `collect()` only when you need data in R
- Filter early in dplyr chains to reduce data transfer

For additional help, see the package documentation: `help(package = "hansardR")`
